{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras02_SaveModel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoheiFukuhara/keras-for-beginner/blob/master/Keras02_SaveModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxQwrFRpIYg4",
        "colab_type": "text"
      },
      "source": [
        "[「【Keras入門(2)】訓練モデル保存(KerasモデルとSavedModel)」](https://qiita.com/FukuharaYohei/items/ac6333391b8abda94bdc)で解説しています。\n",
        "\n",
        "[公式チュートリアル](https://www.tensorflow.org/tutorials/keras/save_and_restore_models)を参考にしました。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XNd8ckxImSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "#tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1UUj0cQIymw",
        "colab_type": "text"
      },
      "source": [
        "# テストデータの作成\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xCNECTmJo90",
        "colab_type": "text"
      },
      "source": [
        "## 説明変数\n",
        "0から1までの乱数で128, 2の配列を作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJbWrvLrItzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_TRAIN = 128\n",
        "data = np.random.rand(NUM_TRAIN,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s1BfGBcKxcM",
        "colab_type": "text"
      },
      "source": [
        "## 目的変数\n",
        "説明変数の各行の2つの値を足した数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxBgqkXvJaXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = (np.sum(data, axis=1) > 1.0) * 1\n",
        "labels = labels.reshape(NUM_TRAIN,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbucAE7KLiLN",
        "colab_type": "text"
      },
      "source": [
        "# モデル定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx2Rq-vqLTvN",
        "colab_type": "code",
        "outputId": "22543914-ac45-43d9-fd83-a484800697c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "# Sequentialモデル使用(Sequentialモデルはレイヤを順に重ねたモデル)\n",
        "model = Sequential()\n",
        "\n",
        "# 全結合層(2層->4層)\n",
        "model.add(Dense(4, input_dim=2, activation=\"tanh\"))\n",
        "\n",
        "# 結合層(4層->1層)：入力次元を省略すると自動的に前の層の出力次元数を引き継ぐ\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# モデルをコンパイル\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 4)                 12        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 17\n",
            "Trainable params: 17\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sqWlwXfOirI",
        "colab_type": "text"
      },
      "source": [
        "#訓練"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiY8i88yl_vD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callbackを定義し、モデル保存の追加\n",
        "li_cb = []\n",
        "li_cb.append(ModelCheckpoint('./model.hdf5', save_best_only=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H250EwhdMI8d",
        "colab_type": "code",
        "outputId": "eb2481c1-1df2-4fdb-e703-d65adfc8c458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10541
        }
      },
      "source": [
        "model.fit(data, labels, nb_epoch=300, validation_split=0.2, callbacks=li_cb)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 102 samples, validate on 26 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/300\n",
            "102/102 [==============================] - 0s 2ms/sample - loss: 0.6595 - acc: 0.5294 - val_loss: 0.6950 - val_acc: 0.3846\n",
            "Epoch 2/300\n",
            "102/102 [==============================] - 0s 214us/sample - loss: 0.6588 - acc: 0.5294 - val_loss: 0.6944 - val_acc: 0.3846\n",
            "Epoch 3/300\n",
            "102/102 [==============================] - 0s 213us/sample - loss: 0.6579 - acc: 0.5294 - val_loss: 0.6925 - val_acc: 0.3846\n",
            "Epoch 4/300\n",
            "102/102 [==============================] - 0s 221us/sample - loss: 0.6570 - acc: 0.5294 - val_loss: 0.6920 - val_acc: 0.3846\n",
            "Epoch 5/300\n",
            "102/102 [==============================] - 0s 202us/sample - loss: 0.6563 - acc: 0.5294 - val_loss: 0.6903 - val_acc: 0.3846\n",
            "Epoch 6/300\n",
            "102/102 [==============================] - 0s 236us/sample - loss: 0.6555 - acc: 0.5686 - val_loss: 0.6891 - val_acc: 0.3846\n",
            "Epoch 7/300\n",
            "102/102 [==============================] - 0s 117us/sample - loss: 0.6547 - acc: 0.5686 - val_loss: 0.6892 - val_acc: 0.3846\n",
            "Epoch 8/300\n",
            "102/102 [==============================] - 0s 89us/sample - loss: 0.6540 - acc: 0.5686 - val_loss: 0.6900 - val_acc: 0.3846\n",
            "Epoch 9/300\n",
            "102/102 [==============================] - 0s 106us/sample - loss: 0.6529 - acc: 0.5588 - val_loss: 0.6894 - val_acc: 0.3846\n",
            "Epoch 10/300\n",
            "102/102 [==============================] - 0s 192us/sample - loss: 0.6521 - acc: 0.5686 - val_loss: 0.6881 - val_acc: 0.3846\n",
            "Epoch 11/300\n",
            "102/102 [==============================] - 0s 230us/sample - loss: 0.6512 - acc: 0.5686 - val_loss: 0.6875 - val_acc: 0.3846\n",
            "Epoch 12/300\n",
            "102/102 [==============================] - 0s 216us/sample - loss: 0.6502 - acc: 0.5784 - val_loss: 0.6870 - val_acc: 0.3846\n",
            "Epoch 13/300\n",
            "102/102 [==============================] - 0s 217us/sample - loss: 0.6493 - acc: 0.5784 - val_loss: 0.6863 - val_acc: 0.3846\n",
            "Epoch 14/300\n",
            "102/102 [==============================] - 0s 125us/sample - loss: 0.6485 - acc: 0.5784 - val_loss: 0.6864 - val_acc: 0.3846\n",
            "Epoch 15/300\n",
            "102/102 [==============================] - 0s 248us/sample - loss: 0.6477 - acc: 0.5784 - val_loss: 0.6852 - val_acc: 0.3846\n",
            "Epoch 16/300\n",
            "102/102 [==============================] - 0s 239us/sample - loss: 0.6470 - acc: 0.5784 - val_loss: 0.6846 - val_acc: 0.3846\n",
            "Epoch 17/300\n",
            "102/102 [==============================] - 0s 227us/sample - loss: 0.6461 - acc: 0.5784 - val_loss: 0.6839 - val_acc: 0.3846\n",
            "Epoch 18/300\n",
            "102/102 [==============================] - 0s 255us/sample - loss: 0.6454 - acc: 0.5882 - val_loss: 0.6827 - val_acc: 0.3846\n",
            "Epoch 19/300\n",
            "102/102 [==============================] - 0s 137us/sample - loss: 0.6447 - acc: 0.6275 - val_loss: 0.6834 - val_acc: 0.3846\n",
            "Epoch 20/300\n",
            "102/102 [==============================] - 0s 212us/sample - loss: 0.6437 - acc: 0.5980 - val_loss: 0.6821 - val_acc: 0.3846\n",
            "Epoch 21/300\n",
            "102/102 [==============================] - 0s 124us/sample - loss: 0.6429 - acc: 0.6275 - val_loss: 0.6821 - val_acc: 0.3846\n",
            "Epoch 22/300\n",
            "102/102 [==============================] - 0s 104us/sample - loss: 0.6421 - acc: 0.6176 - val_loss: 0.6821 - val_acc: 0.3846\n",
            "Epoch 23/300\n",
            "102/102 [==============================] - 0s 135us/sample - loss: 0.6413 - acc: 0.6275 - val_loss: 0.6827 - val_acc: 0.3846\n",
            "Epoch 24/300\n",
            "102/102 [==============================] - 0s 220us/sample - loss: 0.6404 - acc: 0.5980 - val_loss: 0.6820 - val_acc: 0.3846\n",
            "Epoch 25/300\n",
            "102/102 [==============================] - 0s 238us/sample - loss: 0.6395 - acc: 0.5980 - val_loss: 0.6819 - val_acc: 0.3846\n",
            "Epoch 26/300\n",
            "102/102 [==============================] - 0s 206us/sample - loss: 0.6388 - acc: 0.5980 - val_loss: 0.6805 - val_acc: 0.3846\n",
            "Epoch 27/300\n",
            "102/102 [==============================] - 0s 184us/sample - loss: 0.6378 - acc: 0.6176 - val_loss: 0.6791 - val_acc: 0.3846\n",
            "Epoch 28/300\n",
            "102/102 [==============================] - 0s 256us/sample - loss: 0.6371 - acc: 0.6275 - val_loss: 0.6791 - val_acc: 0.3846\n",
            "Epoch 29/300\n",
            "102/102 [==============================] - 0s 203us/sample - loss: 0.6361 - acc: 0.6275 - val_loss: 0.6791 - val_acc: 0.3846\n",
            "Epoch 30/300\n",
            "102/102 [==============================] - 0s 243us/sample - loss: 0.6354 - acc: 0.6275 - val_loss: 0.6784 - val_acc: 0.3846\n",
            "Epoch 31/300\n",
            "102/102 [==============================] - 0s 217us/sample - loss: 0.6344 - acc: 0.6275 - val_loss: 0.6769 - val_acc: 0.3846\n",
            "Epoch 32/300\n",
            "102/102 [==============================] - 0s 209us/sample - loss: 0.6336 - acc: 0.6275 - val_loss: 0.6748 - val_acc: 0.4231\n",
            "Epoch 33/300\n",
            "102/102 [==============================] - 0s 210us/sample - loss: 0.6328 - acc: 0.6373 - val_loss: 0.6748 - val_acc: 0.4231\n",
            "Epoch 34/300\n",
            "102/102 [==============================] - 0s 206us/sample - loss: 0.6320 - acc: 0.6373 - val_loss: 0.6748 - val_acc: 0.4231\n",
            "Epoch 35/300\n",
            "102/102 [==============================] - 0s 109us/sample - loss: 0.6311 - acc: 0.6373 - val_loss: 0.6755 - val_acc: 0.3846\n",
            "Epoch 36/300\n",
            "102/102 [==============================] - 0s 190us/sample - loss: 0.6303 - acc: 0.6275 - val_loss: 0.6747 - val_acc: 0.4231\n",
            "Epoch 37/300\n",
            "102/102 [==============================] - 0s 241us/sample - loss: 0.6294 - acc: 0.6373 - val_loss: 0.6746 - val_acc: 0.4231\n",
            "Epoch 38/300\n",
            "102/102 [==============================] - 0s 221us/sample - loss: 0.6285 - acc: 0.6275 - val_loss: 0.6738 - val_acc: 0.4231\n",
            "Epoch 39/300\n",
            "102/102 [==============================] - 0s 113us/sample - loss: 0.6278 - acc: 0.6373 - val_loss: 0.6744 - val_acc: 0.4231\n",
            "Epoch 40/300\n",
            "102/102 [==============================] - 0s 111us/sample - loss: 0.6270 - acc: 0.6275 - val_loss: 0.6742 - val_acc: 0.4231\n",
            "Epoch 41/300\n",
            "102/102 [==============================] - 0s 221us/sample - loss: 0.6260 - acc: 0.6275 - val_loss: 0.6733 - val_acc: 0.4231\n",
            "Epoch 42/300\n",
            "102/102 [==============================] - 0s 215us/sample - loss: 0.6253 - acc: 0.6275 - val_loss: 0.6723 - val_acc: 0.4231\n",
            "Epoch 43/300\n",
            "102/102 [==============================] - 0s 207us/sample - loss: 0.6244 - acc: 0.6275 - val_loss: 0.6714 - val_acc: 0.4231\n",
            "Epoch 44/300\n",
            "102/102 [==============================] - 0s 215us/sample - loss: 0.6239 - acc: 0.6471 - val_loss: 0.6712 - val_acc: 0.4231\n",
            "Epoch 45/300\n",
            "102/102 [==============================] - 0s 207us/sample - loss: 0.6229 - acc: 0.6471 - val_loss: 0.6702 - val_acc: 0.4231\n",
            "Epoch 46/300\n",
            "102/102 [==============================] - 0s 220us/sample - loss: 0.6221 - acc: 0.6471 - val_loss: 0.6701 - val_acc: 0.4231\n",
            "Epoch 47/300\n",
            "102/102 [==============================] - 0s 212us/sample - loss: 0.6212 - acc: 0.6471 - val_loss: 0.6692 - val_acc: 0.4231\n",
            "Epoch 48/300\n",
            "102/102 [==============================] - 0s 241us/sample - loss: 0.6206 - acc: 0.6471 - val_loss: 0.6675 - val_acc: 0.4231\n",
            "Epoch 49/300\n",
            "102/102 [==============================] - 0s 211us/sample - loss: 0.6199 - acc: 0.6471 - val_loss: 0.6666 - val_acc: 0.4231\n",
            "Epoch 50/300\n",
            "102/102 [==============================] - 0s 214us/sample - loss: 0.6187 - acc: 0.6569 - val_loss: 0.6658 - val_acc: 0.4231\n",
            "Epoch 51/300\n",
            "102/102 [==============================] - 0s 230us/sample - loss: 0.6180 - acc: 0.6569 - val_loss: 0.6657 - val_acc: 0.4231\n",
            "Epoch 52/300\n",
            "102/102 [==============================] - 0s 201us/sample - loss: 0.6171 - acc: 0.6569 - val_loss: 0.6641 - val_acc: 0.4231\n",
            "Epoch 53/300\n",
            "102/102 [==============================] - 0s 212us/sample - loss: 0.6163 - acc: 0.6569 - val_loss: 0.6626 - val_acc: 0.4231\n",
            "Epoch 54/300\n",
            "102/102 [==============================] - 0s 189us/sample - loss: 0.6154 - acc: 0.6569 - val_loss: 0.6616 - val_acc: 0.4231\n",
            "Epoch 55/300\n",
            "102/102 [==============================] - 0s 282us/sample - loss: 0.6146 - acc: 0.6569 - val_loss: 0.6608 - val_acc: 0.4231\n",
            "Epoch 56/300\n",
            "102/102 [==============================] - 0s 210us/sample - loss: 0.6139 - acc: 0.6569 - val_loss: 0.6598 - val_acc: 0.4231\n",
            "Epoch 57/300\n",
            "102/102 [==============================] - 0s 198us/sample - loss: 0.6130 - acc: 0.6569 - val_loss: 0.6598 - val_acc: 0.4231\n",
            "Epoch 58/300\n",
            "102/102 [==============================] - 0s 223us/sample - loss: 0.6122 - acc: 0.6569 - val_loss: 0.6596 - val_acc: 0.4231\n",
            "Epoch 59/300\n",
            "102/102 [==============================] - 0s 103us/sample - loss: 0.6114 - acc: 0.6569 - val_loss: 0.6603 - val_acc: 0.4231\n",
            "Epoch 60/300\n",
            "102/102 [==============================] - 0s 204us/sample - loss: 0.6107 - acc: 0.6569 - val_loss: 0.6586 - val_acc: 0.4231\n",
            "Epoch 61/300\n",
            "102/102 [==============================] - 0s 187us/sample - loss: 0.6099 - acc: 0.6569 - val_loss: 0.6577 - val_acc: 0.4231\n",
            "Epoch 62/300\n",
            "102/102 [==============================] - 0s 225us/sample - loss: 0.6093 - acc: 0.6667 - val_loss: 0.6561 - val_acc: 0.4615\n",
            "Epoch 63/300\n",
            "102/102 [==============================] - 0s 192us/sample - loss: 0.6084 - acc: 0.6765 - val_loss: 0.6547 - val_acc: 0.4615\n",
            "Epoch 64/300\n",
            "102/102 [==============================] - 0s 266us/sample - loss: 0.6074 - acc: 0.7059 - val_loss: 0.6533 - val_acc: 0.4615\n",
            "Epoch 65/300\n",
            "102/102 [==============================] - 0s 231us/sample - loss: 0.6067 - acc: 0.7059 - val_loss: 0.6518 - val_acc: 0.4615\n",
            "Epoch 66/300\n",
            "102/102 [==============================] - 0s 106us/sample - loss: 0.6058 - acc: 0.7255 - val_loss: 0.6526 - val_acc: 0.4615\n",
            "Epoch 67/300\n",
            "102/102 [==============================] - 0s 203us/sample - loss: 0.6049 - acc: 0.7059 - val_loss: 0.6518 - val_acc: 0.4615\n",
            "Epoch 68/300\n",
            "102/102 [==============================] - 0s 227us/sample - loss: 0.6043 - acc: 0.7255 - val_loss: 0.6510 - val_acc: 0.4615\n",
            "Epoch 69/300\n",
            "102/102 [==============================] - 0s 184us/sample - loss: 0.6034 - acc: 0.7255 - val_loss: 0.6495 - val_acc: 0.4615\n",
            "Epoch 70/300\n",
            "102/102 [==============================] - 0s 230us/sample - loss: 0.6026 - acc: 0.7255 - val_loss: 0.6486 - val_acc: 0.4615\n",
            "Epoch 71/300\n",
            "102/102 [==============================] - 0s 238us/sample - loss: 0.6021 - acc: 0.7255 - val_loss: 0.6479 - val_acc: 0.4615\n",
            "Epoch 72/300\n",
            "102/102 [==============================] - 0s 192us/sample - loss: 0.6011 - acc: 0.7255 - val_loss: 0.6463 - val_acc: 0.5000\n",
            "Epoch 73/300\n",
            "102/102 [==============================] - 0s 198us/sample - loss: 0.6004 - acc: 0.7353 - val_loss: 0.6455 - val_acc: 0.5000\n",
            "Epoch 74/300\n",
            "102/102 [==============================] - 0s 215us/sample - loss: 0.5995 - acc: 0.7353 - val_loss: 0.6455 - val_acc: 0.5000\n",
            "Epoch 75/300\n",
            "102/102 [==============================] - 0s 201us/sample - loss: 0.5988 - acc: 0.7353 - val_loss: 0.6432 - val_acc: 0.5000\n",
            "Epoch 76/300\n",
            "102/102 [==============================] - 0s 203us/sample - loss: 0.5980 - acc: 0.7353 - val_loss: 0.6427 - val_acc: 0.5000\n",
            "Epoch 77/300\n",
            "102/102 [==============================] - 0s 124us/sample - loss: 0.5972 - acc: 0.7353 - val_loss: 0.6432 - val_acc: 0.5000\n",
            "Epoch 78/300\n",
            "102/102 [==============================] - 0s 122us/sample - loss: 0.5964 - acc: 0.7353 - val_loss: 0.6432 - val_acc: 0.5000\n",
            "Epoch 79/300\n",
            "102/102 [==============================] - 0s 203us/sample - loss: 0.5955 - acc: 0.7353 - val_loss: 0.6416 - val_acc: 0.5000\n",
            "Epoch 80/300\n",
            "102/102 [==============================] - 0s 216us/sample - loss: 0.5947 - acc: 0.7353 - val_loss: 0.6407 - val_acc: 0.5000\n",
            "Epoch 81/300\n",
            "102/102 [==============================] - 0s 95us/sample - loss: 0.5939 - acc: 0.7353 - val_loss: 0.6407 - val_acc: 0.5000\n",
            "Epoch 82/300\n",
            "102/102 [==============================] - 0s 206us/sample - loss: 0.5931 - acc: 0.7353 - val_loss: 0.6406 - val_acc: 0.5000\n",
            "Epoch 83/300\n",
            "102/102 [==============================] - 0s 225us/sample - loss: 0.5924 - acc: 0.7353 - val_loss: 0.6384 - val_acc: 0.5385\n",
            "Epoch 84/300\n",
            "102/102 [==============================] - 0s 129us/sample - loss: 0.5914 - acc: 0.7353 - val_loss: 0.6390 - val_acc: 0.5000\n",
            "Epoch 85/300\n",
            "102/102 [==============================] - 0s 120us/sample - loss: 0.5905 - acc: 0.7353 - val_loss: 0.6389 - val_acc: 0.5000\n",
            "Epoch 86/300\n",
            "102/102 [==============================] - 0s 196us/sample - loss: 0.5897 - acc: 0.7353 - val_loss: 0.6380 - val_acc: 0.5000\n",
            "Epoch 87/300\n",
            "102/102 [==============================] - 0s 198us/sample - loss: 0.5889 - acc: 0.7353 - val_loss: 0.6372 - val_acc: 0.5000\n",
            "Epoch 88/300\n",
            "102/102 [==============================] - 0s 197us/sample - loss: 0.5883 - acc: 0.7353 - val_loss: 0.6371 - val_acc: 0.5000\n",
            "Epoch 89/300\n",
            "102/102 [==============================] - 0s 209us/sample - loss: 0.5874 - acc: 0.7353 - val_loss: 0.6370 - val_acc: 0.4615\n",
            "Epoch 90/300\n",
            "102/102 [==============================] - 0s 187us/sample - loss: 0.5865 - acc: 0.7353 - val_loss: 0.6368 - val_acc: 0.4615\n",
            "Epoch 91/300\n",
            "102/102 [==============================] - 0s 251us/sample - loss: 0.5860 - acc: 0.7353 - val_loss: 0.6360 - val_acc: 0.4615\n",
            "Epoch 92/300\n",
            "102/102 [==============================] - 0s 223us/sample - loss: 0.5850 - acc: 0.7353 - val_loss: 0.6342 - val_acc: 0.5769\n",
            "Epoch 93/300\n",
            "102/102 [==============================] - 0s 232us/sample - loss: 0.5841 - acc: 0.7353 - val_loss: 0.6333 - val_acc: 0.5769\n",
            "Epoch 94/300\n",
            "102/102 [==============================] - 0s 206us/sample - loss: 0.5834 - acc: 0.7353 - val_loss: 0.6323 - val_acc: 0.5769\n",
            "Epoch 95/300\n",
            "102/102 [==============================] - 0s 213us/sample - loss: 0.5827 - acc: 0.7353 - val_loss: 0.6321 - val_acc: 0.5769\n",
            "Epoch 96/300\n",
            "102/102 [==============================] - 0s 117us/sample - loss: 0.5820 - acc: 0.7353 - val_loss: 0.6329 - val_acc: 0.5769\n",
            "Epoch 97/300\n",
            "102/102 [==============================] - 0s 123us/sample - loss: 0.5814 - acc: 0.7353 - val_loss: 0.6326 - val_acc: 0.5769\n",
            "Epoch 98/300\n",
            "102/102 [==============================] - 0s 205us/sample - loss: 0.5804 - acc: 0.7353 - val_loss: 0.6316 - val_acc: 0.5769\n",
            "Epoch 99/300\n",
            "102/102 [==============================] - 0s 202us/sample - loss: 0.5795 - acc: 0.7353 - val_loss: 0.6313 - val_acc: 0.5769\n",
            "Epoch 100/300\n",
            "102/102 [==============================] - 0s 196us/sample - loss: 0.5789 - acc: 0.7353 - val_loss: 0.6305 - val_acc: 0.5769\n",
            "Epoch 101/300\n",
            "102/102 [==============================] - 0s 207us/sample - loss: 0.5781 - acc: 0.7353 - val_loss: 0.6270 - val_acc: 0.6154\n",
            "Epoch 102/300\n",
            "102/102 [==============================] - 0s 105us/sample - loss: 0.5773 - acc: 0.7451 - val_loss: 0.6276 - val_acc: 0.5769\n",
            "Epoch 103/300\n",
            "102/102 [==============================] - 0s 113us/sample - loss: 0.5765 - acc: 0.7451 - val_loss: 0.6276 - val_acc: 0.5769\n",
            "Epoch 104/300\n",
            "102/102 [==============================] - 0s 200us/sample - loss: 0.5759 - acc: 0.7451 - val_loss: 0.6266 - val_acc: 0.5769\n",
            "Epoch 105/300\n",
            "102/102 [==============================] - 0s 213us/sample - loss: 0.5749 - acc: 0.7451 - val_loss: 0.6256 - val_acc: 0.5769\n",
            "Epoch 106/300\n",
            "102/102 [==============================] - 0s 279us/sample - loss: 0.5741 - acc: 0.7451 - val_loss: 0.6254 - val_acc: 0.5769\n",
            "Epoch 107/300\n",
            "102/102 [==============================] - 0s 127us/sample - loss: 0.5734 - acc: 0.7451 - val_loss: 0.6259 - val_acc: 0.5769\n",
            "Epoch 108/300\n",
            "102/102 [==============================] - 0s 197us/sample - loss: 0.5726 - acc: 0.7353 - val_loss: 0.6242 - val_acc: 0.5769\n",
            "Epoch 109/300\n",
            "102/102 [==============================] - 0s 257us/sample - loss: 0.5719 - acc: 0.7549 - val_loss: 0.6233 - val_acc: 0.5769\n",
            "Epoch 110/300\n",
            "102/102 [==============================] - 0s 185us/sample - loss: 0.5709 - acc: 0.7451 - val_loss: 0.6207 - val_acc: 0.6538\n",
            "Epoch 111/300\n",
            "102/102 [==============================] - 0s 201us/sample - loss: 0.5704 - acc: 0.7549 - val_loss: 0.6197 - val_acc: 0.6538\n",
            "Epoch 112/300\n",
            "102/102 [==============================] - 0s 225us/sample - loss: 0.5693 - acc: 0.7647 - val_loss: 0.6190 - val_acc: 0.6538\n",
            "Epoch 113/300\n",
            "102/102 [==============================] - 0s 212us/sample - loss: 0.5686 - acc: 0.7843 - val_loss: 0.6187 - val_acc: 0.6538\n",
            "Epoch 114/300\n",
            "102/102 [==============================] - 0s 207us/sample - loss: 0.5676 - acc: 0.7745 - val_loss: 0.6186 - val_acc: 0.6538\n",
            "Epoch 115/300\n",
            "102/102 [==============================] - 0s 218us/sample - loss: 0.5669 - acc: 0.7647 - val_loss: 0.6184 - val_acc: 0.6538\n",
            "Epoch 116/300\n",
            "102/102 [==============================] - 0s 206us/sample - loss: 0.5662 - acc: 0.7647 - val_loss: 0.6176 - val_acc: 0.6538\n",
            "Epoch 117/300\n",
            "102/102 [==============================] - 0s 238us/sample - loss: 0.5653 - acc: 0.7745 - val_loss: 0.6158 - val_acc: 0.6538\n",
            "Epoch 118/300\n",
            "102/102 [==============================] - 0s 195us/sample - loss: 0.5645 - acc: 0.7843 - val_loss: 0.6151 - val_acc: 0.6538\n",
            "Epoch 119/300\n",
            "102/102 [==============================] - 0s 119us/sample - loss: 0.5636 - acc: 0.7941 - val_loss: 0.6155 - val_acc: 0.6538\n",
            "Epoch 120/300\n",
            "102/102 [==============================] - 0s 270us/sample - loss: 0.5628 - acc: 0.7745 - val_loss: 0.6131 - val_acc: 0.6538\n",
            "Epoch 121/300\n",
            "102/102 [==============================] - 0s 220us/sample - loss: 0.5622 - acc: 0.8431 - val_loss: 0.6124 - val_acc: 0.6538\n",
            "Epoch 122/300\n",
            "102/102 [==============================] - 0s 129us/sample - loss: 0.5612 - acc: 0.8333 - val_loss: 0.6133 - val_acc: 0.6538\n",
            "Epoch 123/300\n",
            "102/102 [==============================] - 0s 234us/sample - loss: 0.5606 - acc: 0.7843 - val_loss: 0.6123 - val_acc: 0.6538\n",
            "Epoch 124/300\n",
            "102/102 [==============================] - 0s 227us/sample - loss: 0.5597 - acc: 0.8137 - val_loss: 0.6112 - val_acc: 0.6538\n",
            "Epoch 125/300\n",
            "102/102 [==============================] - 0s 260us/sample - loss: 0.5589 - acc: 0.8235 - val_loss: 0.6096 - val_acc: 0.6538\n",
            "Epoch 126/300\n",
            "102/102 [==============================] - 0s 110us/sample - loss: 0.5580 - acc: 0.8431 - val_loss: 0.6097 - val_acc: 0.6538\n",
            "Epoch 127/300\n",
            "102/102 [==============================] - 0s 320us/sample - loss: 0.5573 - acc: 0.8333 - val_loss: 0.6089 - val_acc: 0.6538\n",
            "Epoch 128/300\n",
            "102/102 [==============================] - 0s 215us/sample - loss: 0.5564 - acc: 0.8333 - val_loss: 0.6072 - val_acc: 0.6538\n",
            "Epoch 129/300\n",
            "102/102 [==============================] - 0s 233us/sample - loss: 0.5558 - acc: 0.8529 - val_loss: 0.6065 - val_acc: 0.6538\n",
            "Epoch 130/300\n",
            "102/102 [==============================] - 0s 250us/sample - loss: 0.5548 - acc: 0.8431 - val_loss: 0.6049 - val_acc: 0.6538\n",
            "Epoch 131/300\n",
            "102/102 [==============================] - 0s 249us/sample - loss: 0.5542 - acc: 0.8529 - val_loss: 0.6031 - val_acc: 0.6538\n",
            "Epoch 132/300\n",
            "102/102 [==============================] - 0s 249us/sample - loss: 0.5534 - acc: 0.8627 - val_loss: 0.6025 - val_acc: 0.6538\n",
            "Epoch 133/300\n",
            "102/102 [==============================] - 0s 223us/sample - loss: 0.5527 - acc: 0.8627 - val_loss: 0.6003 - val_acc: 0.6923\n",
            "Epoch 134/300\n",
            "102/102 [==============================] - 0s 223us/sample - loss: 0.5518 - acc: 0.8627 - val_loss: 0.5997 - val_acc: 0.6923\n",
            "Epoch 135/300\n",
            "102/102 [==============================] - 0s 198us/sample - loss: 0.5510 - acc: 0.8627 - val_loss: 0.5988 - val_acc: 0.6923\n",
            "Epoch 136/300\n",
            "102/102 [==============================] - 0s 246us/sample - loss: 0.5502 - acc: 0.8627 - val_loss: 0.5976 - val_acc: 0.6923\n",
            "Epoch 137/300\n",
            "102/102 [==============================] - 0s 237us/sample - loss: 0.5494 - acc: 0.8627 - val_loss: 0.5966 - val_acc: 0.6923\n",
            "Epoch 138/300\n",
            "102/102 [==============================] - 0s 232us/sample - loss: 0.5486 - acc: 0.8627 - val_loss: 0.5945 - val_acc: 0.7308\n",
            "Epoch 139/300\n",
            "102/102 [==============================] - 0s 289us/sample - loss: 0.5478 - acc: 0.8824 - val_loss: 0.5939 - val_acc: 0.7308\n",
            "Epoch 140/300\n",
            "102/102 [==============================] - 0s 274us/sample - loss: 0.5471 - acc: 0.8824 - val_loss: 0.5932 - val_acc: 0.7308\n",
            "Epoch 141/300\n",
            "102/102 [==============================] - 0s 217us/sample - loss: 0.5464 - acc: 0.8922 - val_loss: 0.5918 - val_acc: 0.7692\n",
            "Epoch 142/300\n",
            "102/102 [==============================] - 0s 213us/sample - loss: 0.5457 - acc: 0.8922 - val_loss: 0.5907 - val_acc: 0.8077\n",
            "Epoch 143/300\n",
            "102/102 [==============================] - 0s 127us/sample - loss: 0.5450 - acc: 0.8922 - val_loss: 0.5916 - val_acc: 0.7308\n",
            "Epoch 144/300\n",
            "102/102 [==============================] - 0s 107us/sample - loss: 0.5440 - acc: 0.8922 - val_loss: 0.5915 - val_acc: 0.7308\n",
            "Epoch 145/300\n",
            "102/102 [==============================] - 0s 102us/sample - loss: 0.5432 - acc: 0.8824 - val_loss: 0.5908 - val_acc: 0.7308\n",
            "Epoch 146/300\n",
            "102/102 [==============================] - 0s 103us/sample - loss: 0.5426 - acc: 0.8922 - val_loss: 0.5911 - val_acc: 0.6923\n",
            "Epoch 147/300\n",
            "102/102 [==============================] - 0s 1ms/sample - loss: 0.5416 - acc: 0.8725 - val_loss: 0.5895 - val_acc: 0.7308\n",
            "Epoch 148/300\n",
            "102/102 [==============================] - 0s 230us/sample - loss: 0.5408 - acc: 0.8922 - val_loss: 0.5886 - val_acc: 0.7692\n",
            "Epoch 149/300\n",
            "102/102 [==============================] - 0s 117us/sample - loss: 0.5403 - acc: 0.8824 - val_loss: 0.5889 - val_acc: 0.7308\n",
            "Epoch 150/300\n",
            "102/102 [==============================] - 0s 117us/sample - loss: 0.5395 - acc: 0.8824 - val_loss: 0.5895 - val_acc: 0.6923\n",
            "Epoch 151/300\n",
            "102/102 [==============================] - 0s 200us/sample - loss: 0.5385 - acc: 0.8725 - val_loss: 0.5878 - val_acc: 0.7308\n",
            "Epoch 152/300\n",
            "102/102 [==============================] - 0s 218us/sample - loss: 0.5378 - acc: 0.8922 - val_loss: 0.5866 - val_acc: 0.7692\n",
            "Epoch 153/300\n",
            "102/102 [==============================] - 0s 222us/sample - loss: 0.5369 - acc: 0.8922 - val_loss: 0.5860 - val_acc: 0.7692\n",
            "Epoch 154/300\n",
            "102/102 [==============================] - 0s 235us/sample - loss: 0.5361 - acc: 0.8824 - val_loss: 0.5844 - val_acc: 0.7692\n",
            "Epoch 155/300\n",
            "102/102 [==============================] - 0s 219us/sample - loss: 0.5353 - acc: 0.8922 - val_loss: 0.5828 - val_acc: 0.8077\n",
            "Epoch 156/300\n",
            "102/102 [==============================] - 0s 219us/sample - loss: 0.5346 - acc: 0.8922 - val_loss: 0.5822 - val_acc: 0.8077\n",
            "Epoch 157/300\n",
            "102/102 [==============================] - 0s 117us/sample - loss: 0.5338 - acc: 0.8922 - val_loss: 0.5823 - val_acc: 0.7692\n",
            "Epoch 158/300\n",
            "102/102 [==============================] - 0s 205us/sample - loss: 0.5329 - acc: 0.8922 - val_loss: 0.5816 - val_acc: 0.7692\n",
            "Epoch 159/300\n",
            "102/102 [==============================] - 0s 227us/sample - loss: 0.5322 - acc: 0.8922 - val_loss: 0.5810 - val_acc: 0.7692\n",
            "Epoch 160/300\n",
            "102/102 [==============================] - 0s 210us/sample - loss: 0.5314 - acc: 0.8922 - val_loss: 0.5810 - val_acc: 0.7692\n",
            "Epoch 161/300\n",
            "102/102 [==============================] - 0s 264us/sample - loss: 0.5306 - acc: 0.8922 - val_loss: 0.5795 - val_acc: 0.8077\n",
            "Epoch 162/300\n",
            "102/102 [==============================] - 0s 216us/sample - loss: 0.5299 - acc: 0.8922 - val_loss: 0.5779 - val_acc: 0.8077\n",
            "Epoch 163/300\n",
            "102/102 [==============================] - 0s 224us/sample - loss: 0.5293 - acc: 0.8922 - val_loss: 0.5757 - val_acc: 0.8846\n",
            "Epoch 164/300\n",
            "102/102 [==============================] - 0s 216us/sample - loss: 0.5286 - acc: 0.8922 - val_loss: 0.5755 - val_acc: 0.8846\n",
            "Epoch 165/300\n",
            "102/102 [==============================] - 0s 115us/sample - loss: 0.5277 - acc: 0.8922 - val_loss: 0.5758 - val_acc: 0.8077\n",
            "Epoch 166/300\n",
            "102/102 [==============================] - 0s 115us/sample - loss: 0.5268 - acc: 0.8922 - val_loss: 0.5767 - val_acc: 0.8077\n",
            "Epoch 167/300\n",
            "102/102 [==============================] - 0s 119us/sample - loss: 0.5261 - acc: 0.8922 - val_loss: 0.5761 - val_acc: 0.8077\n",
            "Epoch 168/300\n",
            "102/102 [==============================] - 0s 229us/sample - loss: 0.5254 - acc: 0.8922 - val_loss: 0.5752 - val_acc: 0.8077\n",
            "Epoch 169/300\n",
            "102/102 [==============================] - 0s 214us/sample - loss: 0.5245 - acc: 0.8922 - val_loss: 0.5737 - val_acc: 0.8077\n",
            "Epoch 170/300\n",
            "102/102 [==============================] - 0s 212us/sample - loss: 0.5236 - acc: 0.8922 - val_loss: 0.5721 - val_acc: 0.8846\n",
            "Epoch 171/300\n",
            "102/102 [==============================] - 0s 196us/sample - loss: 0.5228 - acc: 0.8922 - val_loss: 0.5708 - val_acc: 0.8846\n",
            "Epoch 172/300\n",
            "102/102 [==============================] - 0s 204us/sample - loss: 0.5222 - acc: 0.8922 - val_loss: 0.5687 - val_acc: 0.8846\n",
            "Epoch 173/300\n",
            "102/102 [==============================] - 0s 217us/sample - loss: 0.5215 - acc: 0.9020 - val_loss: 0.5677 - val_acc: 0.8846\n",
            "Epoch 174/300\n",
            "102/102 [==============================] - 0s 237us/sample - loss: 0.5207 - acc: 0.9020 - val_loss: 0.5665 - val_acc: 0.8846\n",
            "Epoch 175/300\n",
            "102/102 [==============================] - 0s 220us/sample - loss: 0.5198 - acc: 0.9020 - val_loss: 0.5651 - val_acc: 0.8846\n",
            "Epoch 176/300\n",
            "102/102 [==============================] - 0s 213us/sample - loss: 0.5194 - acc: 0.9118 - val_loss: 0.5639 - val_acc: 0.8846\n",
            "Epoch 177/300\n",
            "102/102 [==============================] - 0s 216us/sample - loss: 0.5187 - acc: 0.9020 - val_loss: 0.5629 - val_acc: 0.8846\n",
            "Epoch 178/300\n",
            "102/102 [==============================] - 0s 202us/sample - loss: 0.5175 - acc: 0.9020 - val_loss: 0.5620 - val_acc: 0.9231\n",
            "Epoch 179/300\n",
            "102/102 [==============================] - 0s 205us/sample - loss: 0.5169 - acc: 0.9118 - val_loss: 0.5608 - val_acc: 0.9231\n",
            "Epoch 180/300\n",
            "102/102 [==============================] - 0s 185us/sample - loss: 0.5166 - acc: 0.9118 - val_loss: 0.5605 - val_acc: 0.9231\n",
            "Epoch 181/300\n",
            "102/102 [==============================] - 0s 213us/sample - loss: 0.5153 - acc: 0.9118 - val_loss: 0.5595 - val_acc: 0.9231\n",
            "Epoch 182/300\n",
            "102/102 [==============================] - 0s 209us/sample - loss: 0.5146 - acc: 0.9118 - val_loss: 0.5594 - val_acc: 0.8846\n",
            "Epoch 183/300\n",
            "102/102 [==============================] - 0s 252us/sample - loss: 0.5136 - acc: 0.9118 - val_loss: 0.5581 - val_acc: 0.9231\n",
            "Epoch 184/300\n",
            "102/102 [==============================] - 0s 126us/sample - loss: 0.5130 - acc: 0.9118 - val_loss: 0.5591 - val_acc: 0.8846\n",
            "Epoch 185/300\n",
            "102/102 [==============================] - 0s 213us/sample - loss: 0.5120 - acc: 0.9020 - val_loss: 0.5579 - val_acc: 0.8846\n",
            "Epoch 186/300\n",
            "102/102 [==============================] - 0s 215us/sample - loss: 0.5111 - acc: 0.9118 - val_loss: 0.5572 - val_acc: 0.8846\n",
            "Epoch 187/300\n",
            "102/102 [==============================] - 0s 226us/sample - loss: 0.5104 - acc: 0.9118 - val_loss: 0.5566 - val_acc: 0.8846\n",
            "Epoch 188/300\n",
            "102/102 [==============================] - 0s 203us/sample - loss: 0.5099 - acc: 0.9118 - val_loss: 0.5550 - val_acc: 0.9231\n",
            "Epoch 189/300\n",
            "102/102 [==============================] - 0s 123us/sample - loss: 0.5091 - acc: 0.9118 - val_loss: 0.5567 - val_acc: 0.8846\n",
            "Epoch 190/300\n",
            "102/102 [==============================] - 0s 109us/sample - loss: 0.5083 - acc: 0.9020 - val_loss: 0.5561 - val_acc: 0.8846\n",
            "Epoch 191/300\n",
            "102/102 [==============================] - 0s 99us/sample - loss: 0.5073 - acc: 0.9020 - val_loss: 0.5559 - val_acc: 0.8846\n",
            "Epoch 192/300\n",
            "102/102 [==============================] - 0s 108us/sample - loss: 0.5066 - acc: 0.9020 - val_loss: 0.5560 - val_acc: 0.8846\n",
            "Epoch 193/300\n",
            "102/102 [==============================] - 0s 99us/sample - loss: 0.5057 - acc: 0.9020 - val_loss: 0.5565 - val_acc: 0.8846\n",
            "Epoch 194/300\n",
            "102/102 [==============================] - 0s 104us/sample - loss: 0.5048 - acc: 0.9020 - val_loss: 0.5573 - val_acc: 0.8462\n",
            "Epoch 195/300\n",
            "102/102 [==============================] - 0s 102us/sample - loss: 0.5040 - acc: 0.9020 - val_loss: 0.5571 - val_acc: 0.8462\n",
            "Epoch 196/300\n",
            "102/102 [==============================] - 0s 134us/sample - loss: 0.5032 - acc: 0.9020 - val_loss: 0.5588 - val_acc: 0.8077\n",
            "Epoch 197/300\n",
            "102/102 [==============================] - 0s 129us/sample - loss: 0.5025 - acc: 0.8922 - val_loss: 0.5578 - val_acc: 0.8077\n",
            "Epoch 198/300\n",
            "102/102 [==============================] - 0s 105us/sample - loss: 0.5021 - acc: 0.8922 - val_loss: 0.5568 - val_acc: 0.8077\n",
            "Epoch 199/300\n",
            "102/102 [==============================] - 0s 132us/sample - loss: 0.5010 - acc: 0.8922 - val_loss: 0.5556 - val_acc: 0.8077\n",
            "Epoch 200/300\n",
            "102/102 [==============================] - 0s 104us/sample - loss: 0.5004 - acc: 0.8922 - val_loss: 0.5563 - val_acc: 0.8077\n",
            "Epoch 201/300\n",
            "102/102 [==============================] - 0s 129us/sample - loss: 0.4996 - acc: 0.8922 - val_loss: 0.5555 - val_acc: 0.8077\n",
            "Epoch 202/300\n",
            "102/102 [==============================] - 0s 220us/sample - loss: 0.4989 - acc: 0.8922 - val_loss: 0.5529 - val_acc: 0.8462\n",
            "Epoch 203/300\n",
            "102/102 [==============================] - 0s 215us/sample - loss: 0.4980 - acc: 0.9020 - val_loss: 0.5521 - val_acc: 0.8462\n",
            "Epoch 204/300\n",
            "102/102 [==============================] - 0s 199us/sample - loss: 0.4972 - acc: 0.9020 - val_loss: 0.5509 - val_acc: 0.8462\n",
            "Epoch 205/300\n",
            "102/102 [==============================] - 0s 194us/sample - loss: 0.4967 - acc: 0.9020 - val_loss: 0.5485 - val_acc: 0.8846\n",
            "Epoch 206/300\n",
            "102/102 [==============================] - 0s 225us/sample - loss: 0.4958 - acc: 0.9020 - val_loss: 0.5482 - val_acc: 0.8846\n",
            "Epoch 207/300\n",
            "102/102 [==============================] - 0s 229us/sample - loss: 0.4951 - acc: 0.9020 - val_loss: 0.5469 - val_acc: 0.8846\n",
            "Epoch 208/300\n",
            "102/102 [==============================] - 0s 218us/sample - loss: 0.4944 - acc: 0.9118 - val_loss: 0.5453 - val_acc: 0.8846\n",
            "Epoch 209/300\n",
            "102/102 [==============================] - 0s 213us/sample - loss: 0.4935 - acc: 0.9118 - val_loss: 0.5445 - val_acc: 0.8846\n",
            "Epoch 210/300\n",
            "102/102 [==============================] - 0s 208us/sample - loss: 0.4928 - acc: 0.9118 - val_loss: 0.5439 - val_acc: 0.8846\n",
            "Epoch 211/300\n",
            "102/102 [==============================] - 0s 220us/sample - loss: 0.4920 - acc: 0.9118 - val_loss: 0.5436 - val_acc: 0.8846\n",
            "Epoch 212/300\n",
            "102/102 [==============================] - 0s 219us/sample - loss: 0.4913 - acc: 0.9118 - val_loss: 0.5428 - val_acc: 0.8846\n",
            "Epoch 213/300\n",
            "102/102 [==============================] - 0s 209us/sample - loss: 0.4905 - acc: 0.9118 - val_loss: 0.5414 - val_acc: 0.8846\n",
            "Epoch 214/300\n",
            "102/102 [==============================] - 0s 200us/sample - loss: 0.4898 - acc: 0.9118 - val_loss: 0.5395 - val_acc: 0.8846\n",
            "Epoch 215/300\n",
            "102/102 [==============================] - 0s 143us/sample - loss: 0.4890 - acc: 0.9118 - val_loss: 0.5397 - val_acc: 0.8846\n",
            "Epoch 216/300\n",
            "102/102 [==============================] - 0s 218us/sample - loss: 0.4883 - acc: 0.9118 - val_loss: 0.5367 - val_acc: 0.9231\n",
            "Epoch 217/300\n",
            "102/102 [==============================] - 0s 119us/sample - loss: 0.4876 - acc: 0.9118 - val_loss: 0.5367 - val_acc: 0.8846\n",
            "Epoch 218/300\n",
            "102/102 [==============================] - 0s 100us/sample - loss: 0.4869 - acc: 0.9118 - val_loss: 0.5373 - val_acc: 0.8846\n",
            "Epoch 219/300\n",
            "102/102 [==============================] - 0s 248us/sample - loss: 0.4860 - acc: 0.9118 - val_loss: 0.5353 - val_acc: 0.8846\n",
            "Epoch 220/300\n",
            "102/102 [==============================] - 0s 197us/sample - loss: 0.4852 - acc: 0.9118 - val_loss: 0.5353 - val_acc: 0.8846\n",
            "Epoch 221/300\n",
            "102/102 [==============================] - 0s 189us/sample - loss: 0.4844 - acc: 0.9118 - val_loss: 0.5348 - val_acc: 0.8846\n",
            "Epoch 222/300\n",
            "102/102 [==============================] - 0s 180us/sample - loss: 0.4838 - acc: 0.9118 - val_loss: 0.5325 - val_acc: 0.9231\n",
            "Epoch 223/300\n",
            "102/102 [==============================] - 0s 211us/sample - loss: 0.4831 - acc: 0.9118 - val_loss: 0.5310 - val_acc: 0.9231\n",
            "Epoch 224/300\n",
            "102/102 [==============================] - 0s 223us/sample - loss: 0.4824 - acc: 0.9118 - val_loss: 0.5298 - val_acc: 0.9231\n",
            "Epoch 225/300\n",
            "102/102 [==============================] - 0s 209us/sample - loss: 0.4818 - acc: 0.9118 - val_loss: 0.5286 - val_acc: 0.9231\n",
            "Epoch 226/300\n",
            "102/102 [==============================] - 0s 194us/sample - loss: 0.4811 - acc: 0.9118 - val_loss: 0.5286 - val_acc: 0.9231\n",
            "Epoch 227/300\n",
            "102/102 [==============================] - 0s 234us/sample - loss: 0.4802 - acc: 0.9118 - val_loss: 0.5283 - val_acc: 0.9231\n",
            "Epoch 228/300\n",
            "102/102 [==============================] - 0s 194us/sample - loss: 0.4795 - acc: 0.9118 - val_loss: 0.5262 - val_acc: 0.9231\n",
            "Epoch 229/300\n",
            "102/102 [==============================] - 0s 228us/sample - loss: 0.4788 - acc: 0.9118 - val_loss: 0.5250 - val_acc: 0.9231\n",
            "Epoch 230/300\n",
            "102/102 [==============================] - 0s 126us/sample - loss: 0.4781 - acc: 0.9118 - val_loss: 0.5261 - val_acc: 0.9231\n",
            "Epoch 231/300\n",
            "102/102 [==============================] - 0s 214us/sample - loss: 0.4772 - acc: 0.9118 - val_loss: 0.5245 - val_acc: 0.9231\n",
            "Epoch 232/300\n",
            "102/102 [==============================] - 0s 129us/sample - loss: 0.4765 - acc: 0.9118 - val_loss: 0.5249 - val_acc: 0.9231\n",
            "Epoch 233/300\n",
            "102/102 [==============================] - 0s 205us/sample - loss: 0.4757 - acc: 0.9118 - val_loss: 0.5228 - val_acc: 0.9231\n",
            "Epoch 234/300\n",
            "102/102 [==============================] - 0s 112us/sample - loss: 0.4750 - acc: 0.9118 - val_loss: 0.5230 - val_acc: 0.9231\n",
            "Epoch 235/300\n",
            "102/102 [==============================] - 0s 239us/sample - loss: 0.4743 - acc: 0.9118 - val_loss: 0.5227 - val_acc: 0.9231\n",
            "Epoch 236/300\n",
            "102/102 [==============================] - 0s 115us/sample - loss: 0.4736 - acc: 0.9118 - val_loss: 0.5236 - val_acc: 0.8846\n",
            "Epoch 237/300\n",
            "102/102 [==============================] - 0s 123us/sample - loss: 0.4726 - acc: 0.9118 - val_loss: 0.5239 - val_acc: 0.8846\n",
            "Epoch 238/300\n",
            "102/102 [==============================] - 0s 201us/sample - loss: 0.4719 - acc: 0.9118 - val_loss: 0.5222 - val_acc: 0.8846\n",
            "Epoch 239/300\n",
            "102/102 [==============================] - 0s 226us/sample - loss: 0.4713 - acc: 0.9118 - val_loss: 0.5210 - val_acc: 0.8846\n",
            "Epoch 240/300\n",
            "102/102 [==============================] - 0s 132us/sample - loss: 0.4706 - acc: 0.9118 - val_loss: 0.5223 - val_acc: 0.8846\n",
            "Epoch 241/300\n",
            "102/102 [==============================] - 0s 199us/sample - loss: 0.4697 - acc: 0.9118 - val_loss: 0.5209 - val_acc: 0.8846\n",
            "Epoch 242/300\n",
            "102/102 [==============================] - 0s 216us/sample - loss: 0.4690 - acc: 0.9118 - val_loss: 0.5209 - val_acc: 0.8846\n",
            "Epoch 243/300\n",
            "102/102 [==============================] - 0s 123us/sample - loss: 0.4683 - acc: 0.9118 - val_loss: 0.5216 - val_acc: 0.8846\n",
            "Epoch 244/300\n",
            "102/102 [==============================] - 0s 106us/sample - loss: 0.4676 - acc: 0.9118 - val_loss: 0.5213 - val_acc: 0.8846\n",
            "Epoch 245/300\n",
            "102/102 [==============================] - 0s 200us/sample - loss: 0.4670 - acc: 0.9118 - val_loss: 0.5198 - val_acc: 0.8846\n",
            "Epoch 246/300\n",
            "102/102 [==============================] - 0s 189us/sample - loss: 0.4661 - acc: 0.9118 - val_loss: 0.5197 - val_acc: 0.8846\n",
            "Epoch 247/300\n",
            "102/102 [==============================] - 0s 225us/sample - loss: 0.4655 - acc: 0.9118 - val_loss: 0.5190 - val_acc: 0.8846\n",
            "Epoch 248/300\n",
            "102/102 [==============================] - 0s 247us/sample - loss: 0.4647 - acc: 0.9118 - val_loss: 0.5182 - val_acc: 0.8846\n",
            "Epoch 249/300\n",
            "102/102 [==============================] - 0s 234us/sample - loss: 0.4639 - acc: 0.9118 - val_loss: 0.5167 - val_acc: 0.8846\n",
            "Epoch 250/300\n",
            "102/102 [==============================] - 0s 125us/sample - loss: 0.4632 - acc: 0.9118 - val_loss: 0.5182 - val_acc: 0.8846\n",
            "Epoch 251/300\n",
            "102/102 [==============================] - 0s 226us/sample - loss: 0.4624 - acc: 0.9118 - val_loss: 0.5166 - val_acc: 0.8846\n",
            "Epoch 252/300\n",
            "102/102 [==============================] - 0s 210us/sample - loss: 0.4618 - acc: 0.9118 - val_loss: 0.5147 - val_acc: 0.8846\n",
            "Epoch 253/300\n",
            "102/102 [==============================] - 0s 129us/sample - loss: 0.4609 - acc: 0.9118 - val_loss: 0.5155 - val_acc: 0.8846\n",
            "Epoch 254/300\n",
            "102/102 [==============================] - 0s 197us/sample - loss: 0.4601 - acc: 0.9118 - val_loss: 0.5147 - val_acc: 0.8846\n",
            "Epoch 255/300\n",
            "102/102 [==============================] - 0s 118us/sample - loss: 0.4594 - acc: 0.9118 - val_loss: 0.5149 - val_acc: 0.8846\n",
            "Epoch 256/300\n",
            "102/102 [==============================] - 0s 120us/sample - loss: 0.4587 - acc: 0.9118 - val_loss: 0.5148 - val_acc: 0.8846\n",
            "Epoch 257/300\n",
            "102/102 [==============================] - 0s 253us/sample - loss: 0.4580 - acc: 0.9118 - val_loss: 0.5140 - val_acc: 0.8846\n",
            "Epoch 258/300\n",
            "102/102 [==============================] - 0s 197us/sample - loss: 0.4573 - acc: 0.9118 - val_loss: 0.5133 - val_acc: 0.8846\n",
            "Epoch 259/300\n",
            "102/102 [==============================] - 0s 200us/sample - loss: 0.4566 - acc: 0.9118 - val_loss: 0.5123 - val_acc: 0.8846\n",
            "Epoch 260/300\n",
            "102/102 [==============================] - 0s 198us/sample - loss: 0.4559 - acc: 0.9118 - val_loss: 0.5101 - val_acc: 0.8846\n",
            "Epoch 261/300\n",
            "102/102 [==============================] - 0s 207us/sample - loss: 0.4551 - acc: 0.9118 - val_loss: 0.5092 - val_acc: 0.8846\n",
            "Epoch 262/300\n",
            "102/102 [==============================] - 0s 219us/sample - loss: 0.4544 - acc: 0.9118 - val_loss: 0.5085 - val_acc: 0.8846\n",
            "Epoch 263/300\n",
            "102/102 [==============================] - 0s 208us/sample - loss: 0.4540 - acc: 0.9118 - val_loss: 0.5064 - val_acc: 0.8846\n",
            "Epoch 264/300\n",
            "102/102 [==============================] - 0s 144us/sample - loss: 0.4533 - acc: 0.9118 - val_loss: 0.5075 - val_acc: 0.8846\n",
            "Epoch 265/300\n",
            "102/102 [==============================] - 0s 210us/sample - loss: 0.4524 - acc: 0.9118 - val_loss: 0.5063 - val_acc: 0.8846\n",
            "Epoch 266/300\n",
            "102/102 [==============================] - 0s 211us/sample - loss: 0.4515 - acc: 0.9118 - val_loss: 0.5049 - val_acc: 0.8846\n",
            "Epoch 267/300\n",
            "102/102 [==============================] - 0s 206us/sample - loss: 0.4508 - acc: 0.9118 - val_loss: 0.5045 - val_acc: 0.8846\n",
            "Epoch 268/300\n",
            "102/102 [==============================] - 0s 233us/sample - loss: 0.4501 - acc: 0.9118 - val_loss: 0.5035 - val_acc: 0.8846\n",
            "Epoch 269/300\n",
            "102/102 [==============================] - 0s 142us/sample - loss: 0.4494 - acc: 0.9118 - val_loss: 0.5038 - val_acc: 0.8846\n",
            "Epoch 270/300\n",
            "102/102 [==============================] - 0s 196us/sample - loss: 0.4487 - acc: 0.9118 - val_loss: 0.5021 - val_acc: 0.8846\n",
            "Epoch 271/300\n",
            "102/102 [==============================] - 0s 119us/sample - loss: 0.4481 - acc: 0.9118 - val_loss: 0.5037 - val_acc: 0.8846\n",
            "Epoch 272/300\n",
            "102/102 [==============================] - 0s 206us/sample - loss: 0.4473 - acc: 0.9118 - val_loss: 0.5017 - val_acc: 0.8846\n",
            "Epoch 273/300\n",
            "102/102 [==============================] - 0s 224us/sample - loss: 0.4466 - acc: 0.9118 - val_loss: 0.5015 - val_acc: 0.8846\n",
            "Epoch 274/300\n",
            "102/102 [==============================] - 0s 213us/sample - loss: 0.4459 - acc: 0.9118 - val_loss: 0.5012 - val_acc: 0.8846\n",
            "Epoch 275/300\n",
            "102/102 [==============================] - 0s 102us/sample - loss: 0.4453 - acc: 0.9118 - val_loss: 0.5013 - val_acc: 0.8846\n",
            "Epoch 276/300\n",
            "102/102 [==============================] - 0s 195us/sample - loss: 0.4445 - acc: 0.9118 - val_loss: 0.4995 - val_acc: 0.8846\n",
            "Epoch 277/300\n",
            "102/102 [==============================] - 0s 243us/sample - loss: 0.4439 - acc: 0.9118 - val_loss: 0.4977 - val_acc: 0.8846\n",
            "Epoch 278/300\n",
            "102/102 [==============================] - 0s 197us/sample - loss: 0.4432 - acc: 0.9118 - val_loss: 0.4975 - val_acc: 0.8846\n",
            "Epoch 279/300\n",
            "102/102 [==============================] - 0s 208us/sample - loss: 0.4425 - acc: 0.9118 - val_loss: 0.4969 - val_acc: 0.8846\n",
            "Epoch 280/300\n",
            "102/102 [==============================] - 0s 130us/sample - loss: 0.4418 - acc: 0.9118 - val_loss: 0.4971 - val_acc: 0.8846\n",
            "Epoch 281/300\n",
            "102/102 [==============================] - 0s 180us/sample - loss: 0.4411 - acc: 0.9118 - val_loss: 0.4966 - val_acc: 0.8846\n",
            "Epoch 282/300\n",
            "102/102 [==============================] - 0s 189us/sample - loss: 0.4404 - acc: 0.9118 - val_loss: 0.4950 - val_acc: 0.8846\n",
            "Epoch 283/300\n",
            "102/102 [==============================] - 0s 143us/sample - loss: 0.4398 - acc: 0.9118 - val_loss: 0.4953 - val_acc: 0.8846\n",
            "Epoch 284/300\n",
            "102/102 [==============================] - 0s 116us/sample - loss: 0.4391 - acc: 0.9118 - val_loss: 0.4955 - val_acc: 0.8846\n",
            "Epoch 285/300\n",
            "102/102 [==============================] - 0s 98us/sample - loss: 0.4383 - acc: 0.9118 - val_loss: 0.4951 - val_acc: 0.8846\n",
            "Epoch 286/300\n",
            "102/102 [==============================] - 0s 105us/sample - loss: 0.4377 - acc: 0.9118 - val_loss: 0.4950 - val_acc: 0.8846\n",
            "Epoch 287/300\n",
            "102/102 [==============================] - 0s 194us/sample - loss: 0.4369 - acc: 0.9118 - val_loss: 0.4931 - val_acc: 0.8846\n",
            "Epoch 288/300\n",
            "102/102 [==============================] - 0s 220us/sample - loss: 0.4362 - acc: 0.9118 - val_loss: 0.4909 - val_acc: 0.8846\n",
            "Epoch 289/300\n",
            "102/102 [==============================] - 0s 221us/sample - loss: 0.4356 - acc: 0.9118 - val_loss: 0.4905 - val_acc: 0.8846\n",
            "Epoch 290/300\n",
            "102/102 [==============================] - 0s 220us/sample - loss: 0.4348 - acc: 0.9118 - val_loss: 0.4904 - val_acc: 0.8846\n",
            "Epoch 291/300\n",
            "102/102 [==============================] - 0s 95us/sample - loss: 0.4342 - acc: 0.9118 - val_loss: 0.4907 - val_acc: 0.8846\n",
            "Epoch 292/300\n",
            "102/102 [==============================] - 0s 162us/sample - loss: 0.4336 - acc: 0.9118 - val_loss: 0.4921 - val_acc: 0.8846\n",
            "Epoch 293/300\n",
            "102/102 [==============================] - 0s 179us/sample - loss: 0.4329 - acc: 0.9118 - val_loss: 0.4919 - val_acc: 0.8846\n",
            "Epoch 294/300\n",
            "102/102 [==============================] - 0s 149us/sample - loss: 0.4323 - acc: 0.9118 - val_loss: 0.4921 - val_acc: 0.8462\n",
            "Epoch 295/300\n",
            "102/102 [==============================] - 0s 124us/sample - loss: 0.4318 - acc: 0.9118 - val_loss: 0.4915 - val_acc: 0.8462\n",
            "Epoch 296/300\n",
            "102/102 [==============================] - 0s 197us/sample - loss: 0.4310 - acc: 0.9118 - val_loss: 0.4886 - val_acc: 0.8846\n",
            "Epoch 297/300\n",
            "102/102 [==============================] - 0s 110us/sample - loss: 0.4304 - acc: 0.9118 - val_loss: 0.4905 - val_acc: 0.8462\n",
            "Epoch 298/300\n",
            "102/102 [==============================] - 0s 107us/sample - loss: 0.4298 - acc: 0.9118 - val_loss: 0.4898 - val_acc: 0.8462\n",
            "Epoch 299/300\n",
            "102/102 [==============================] - 0s 133us/sample - loss: 0.4291 - acc: 0.9118 - val_loss: 0.4893 - val_acc: 0.8846\n",
            "Epoch 300/300\n",
            "102/102 [==============================] - 0s 96us/sample - loss: 0.4285 - acc: 0.9118 - val_loss: 0.4895 - val_acc: 0.8462\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb14ee79c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg_u72Bud5AM",
        "colab_type": "code",
        "outputId": "6dc7f3c2-1f46-4d2a-e808-a0911632c3fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "source": [
        "# SavedModelフォーマットで保存(TensorFlow2.0ではexport_saved_modelを使う\n",
        "# https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/experimental/export_saved_model\n",
        "tf.contrib.saved_model.save_keras_model(model, './models/keras_export')\n",
        "\n",
        "# Eager Executionでないとserving_only=Trueは失敗\n",
        "#tf.contrib.saved_model.save_keras_model(model, './models/keras_export', serving_only=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.SGD object at 0x7fb14ee85ef0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
            "WARNING:tensorflow:Model was compiled with an optimizer, but the optimizer is not from `tf.train` (e.g. `tf.train.AdagradOptimizer`). Only the serving graph was exported. The train and evaluate graphs were not added to the SavedModel.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: ./models/keras_export/1558534626/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'./models/keras_export/1558534626'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrYz9ofK0PLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "626fcfd5-8434-42e1-b0c5-c54e9129ff1e"
      },
      "source": [
        "# フォルダ名が動的に生成されるので、都度変更が必要\n",
        "!saved_model_cli show --all --dir ./models/keras_export/1558534626"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['__saved_model_init_op']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['__saved_model_init_op'] tensor_info:\n",
            "        dtype: DT_INVALID\n",
            "        shape: unknown_rank\n",
            "        name: init_1\n",
            "  Method name is: \n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['dense_input'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 2)\n",
            "        name: dense_input:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['dense_1'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 1)\n",
            "        name: dense_1/Sigmoid:0\n",
            "  Method name is: tensorflow/serving/predict\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRFgZTmmg2qK",
        "colab_type": "text"
      },
      "source": [
        "# テスト実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XaxVdTKOk9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_TEST = 50\n",
        "test_data = np.random.rand(NUM_TEST,2)\n",
        "test_labels = (np.sum(test_data, axis=1) > 1.0) * 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZPJ8RoKOzaZ",
        "colab_type": "code",
        "outputId": "a6277db2-c694-4e0d-dcfa-6dd64c9893ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "predict = ((model.predict(test_data) > 0.5) * 1).reshape(NUM_TEST)\n",
        "print(predict)\n",
        "print(test_labels)\n",
        "print(\"Accuracy:\",sum(predict == test_labels) / NUM_TEST)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 1]\n",
            "[1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 1]\n",
            "Accuracy: 0.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFTWhblcrilM",
        "colab_type": "text"
      },
      "source": [
        "#Keras モデルをロードしてSavedModelフォーマットで保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6AK_vVXhgu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデルを初期化してロード\n",
        "model = None\n",
        "model = load_model('./model.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLkCyjTGqwam",
        "colab_type": "code",
        "outputId": "679bc6db-348d-4283-eb7e-0fcf04532a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "tf.contrib.saved_model.save_keras_model(model, './models/keras_export')\n",
        "\n",
        "# Eager Executionでないとserving_only=Trueは失敗\n",
        "#tf.contrib.saved_model.save_keras_model(model, './models/keras_export', serving_only=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.SGD object at 0x7fb13ba24d30>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "WARNING:tensorflow:Model was compiled with an optimizer, but the optimizer is not from `tf.train` (e.g. `tf.train.AdagradOptimizer`). Only the serving graph was exported. The train and evaluate graphs were not added to the SavedModel.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: ./models/keras_export/1558534976/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'./models/keras_export/1558534976'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8FXwRbvrS-v",
        "colab_type": "code",
        "outputId": "a5b52708-a600-40a6-da25-99be6aefb05d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6240
        }
      },
      "source": [
        "# ライブラリバージョン確認\n",
        "!pip freeze"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "absl-py==0.7.1\n",
            "alabaster==0.7.12\n",
            "albumentations==0.1.12\n",
            "altair==3.0.1\n",
            "astor==0.7.1\n",
            "astropy==3.0.5\n",
            "atari-py==0.1.7\n",
            "atomicwrites==1.3.0\n",
            "attrs==19.1.0\n",
            "audioread==2.1.7\n",
            "autograd==1.2\n",
            "Babel==2.6.0\n",
            "backcall==0.1.0\n",
            "backports.tempfile==1.0\n",
            "backports.weakref==1.0.post1\n",
            "beautifulsoup4==4.6.3\n",
            "bleach==3.1.0\n",
            "bokeh==1.0.4\n",
            "boto==2.49.0\n",
            "boto3==1.9.150\n",
            "botocore==1.12.150\n",
            "Bottleneck==1.2.1\n",
            "branca==0.3.1\n",
            "bs4==0.0.1\n",
            "bz2file==0.98\n",
            "cachetools==3.1.0\n",
            "certifi==2019.3.9\n",
            "cffi==1.12.3\n",
            "chainer==5.4.0\n",
            "chardet==3.0.4\n",
            "Click==7.0\n",
            "cloudpickle==0.6.1\n",
            "cmake==3.12.0\n",
            "colorlover==0.3.0\n",
            "community==1.0.0b1\n",
            "contextlib2==0.5.5\n",
            "convertdate==2.1.3\n",
            "coverage==3.7.1\n",
            "coveralls==0.5\n",
            "crcmod==1.7\n",
            "cufflinks==0.14.6\n",
            "cvxopt==1.2.3\n",
            "cvxpy==1.0.15\n",
            "cycler==0.10.0\n",
            "cymem==2.0.2\n",
            "Cython==0.29.7\n",
            "cytoolz==0.9.0.1\n",
            "daft==0.0.4\n",
            "dask==1.1.5\n",
            "dataclasses==0.6\n",
            "datascience==0.10.6\n",
            "decorator==4.4.0\n",
            "defusedxml==0.6.0\n",
            "dill==0.2.9\n",
            "distributed==1.25.3\n",
            "Django==2.2.1\n",
            "dlib==19.16.0\n",
            "dm-sonnet==1.32\n",
            "docopt==0.6.2\n",
            "docutils==0.14\n",
            "dopamine-rl==1.0.5\n",
            "easydict==1.9\n",
            "ecos==2.0.7.post1\n",
            "editdistance==0.5.3\n",
            "en-core-web-sm==2.0.0\n",
            "entrypoints==0.3\n",
            "enum34==1.1.6\n",
            "ephem==3.7.6.0\n",
            "et-xmlfile==1.0.1\n",
            "fa2==0.3.5\n",
            "fancyimpute==0.4.3\n",
            "fastai==1.0.52\n",
            "fastcache==1.1.0\n",
            "fastdtw==0.3.2\n",
            "fastprogress==0.1.21\n",
            "fastrlock==0.4\n",
            "fbprophet==0.5\n",
            "featuretools==0.4.1\n",
            "filelock==3.0.10\n",
            "fix-yahoo-finance==0.0.22\n",
            "Flask==1.0.3\n",
            "folium==0.8.3\n",
            "future==0.16.0\n",
            "gast==0.2.2\n",
            "GDAL==2.2.2\n",
            "gdown==3.6.4\n",
            "gensim==3.6.0\n",
            "geographiclib==1.49\n",
            "geopy==1.17.0\n",
            "gevent==1.4.0\n",
            "gin-config==0.1.4\n",
            "glob2==0.6\n",
            "google==2.0.2\n",
            "google-api-core==1.11.0\n",
            "google-api-python-client==1.6.7\n",
            "google-auth==1.4.2\n",
            "google-auth-httplib2==0.0.3\n",
            "google-auth-oauthlib==0.3.0\n",
            "google-cloud-bigquery==1.8.1\n",
            "google-cloud-core==0.29.1\n",
            "google-cloud-language==1.0.2\n",
            "google-cloud-storage==1.13.2\n",
            "google-cloud-translate==1.3.3\n",
            "google-colab==1.0.0\n",
            "google-resumable-media==0.3.2\n",
            "googleapis-common-protos==1.5.10\n",
            "googledrivedownloader==0.3\n",
            "graph-nets==1.0.4\n",
            "graphviz==0.10.1\n",
            "greenlet==0.4.15\n",
            "grpcio==1.15.0\n",
            "gspread==3.0.1\n",
            "gspread-dataframe==3.0.2\n",
            "gunicorn==19.9.0\n",
            "gym==0.10.11\n",
            "h5py==2.8.0\n",
            "HeapDict==1.0.0\n",
            "holidays==0.9.10\n",
            "html5lib==1.0.1\n",
            "httpimport==0.5.16\n",
            "httplib2==0.11.3\n",
            "humanize==0.5.1\n",
            "hyperopt==0.1.2\n",
            "ideep4py==2.0.0.post3\n",
            "idna==2.8\n",
            "image==1.5.27\n",
            "imageio==2.4.1\n",
            "imagesize==1.1.0\n",
            "imbalanced-learn==0.4.3\n",
            "imblearn==0.0\n",
            "imgaug==0.2.9\n",
            "imutils==0.5.2\n",
            "inflect==2.1.0\n",
            "intel-openmp==2019.0\n",
            "intervaltree==2.1.0\n",
            "ipykernel==4.6.1\n",
            "ipython==5.5.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.3.9\n",
            "ipywidgets==7.4.2\n",
            "itsdangerous==1.1.0\n",
            "jdcal==1.4.1\n",
            "jedi==0.13.3\n",
            "jieba==0.39\n",
            "Jinja2==2.10.1\n",
            "jmespath==0.9.4\n",
            "joblib==0.12.5\n",
            "jpeg4py==0.1.4\n",
            "jsonschema==2.6.0\n",
            "jupyter==1.0.0\n",
            "jupyter-client==5.2.4\n",
            "jupyter-console==6.0.0\n",
            "jupyter-core==4.4.0\n",
            "kaggle==1.5.3\n",
            "kapre==0.1.3.1\n",
            "Keras==2.2.4\n",
            "Keras-Applications==1.0.7\n",
            "Keras-Preprocessing==1.0.9\n",
            "keras-vis==0.4.1\n",
            "kiwisolver==1.1.0\n",
            "knnimpute==0.1.0\n",
            "librosa==0.6.3\n",
            "lightgbm==2.2.3\n",
            "llvmlite==0.28.0\n",
            "lmdb==0.94\n",
            "lucid==0.3.8\n",
            "lunardate==0.2.0\n",
            "lxml==4.2.6\n",
            "magenta==0.3.19\n",
            "Markdown==3.1\n",
            "MarkupSafe==1.1.1\n",
            "matplotlib==3.0.3\n",
            "matplotlib-venn==0.11.5\n",
            "mesh-tensorflow==0.0.5\n",
            "mido==1.2.6\n",
            "mir-eval==0.5\n",
            "missingno==0.4.1\n",
            "mistune==0.8.4\n",
            "mkl==2019.0\n",
            "mlxtend==0.14.0\n",
            "mock==3.0.5\n",
            "more-itertools==7.0.0\n",
            "moviepy==0.2.3.5\n",
            "mpi4py==3.0.1\n",
            "mpmath==1.1.0\n",
            "msgpack==0.5.6\n",
            "msgpack-numpy==0.4.3.2\n",
            "multiprocess==0.70.7\n",
            "multitasking==0.0.8\n",
            "murmurhash==1.0.2\n",
            "music21==5.5.0\n",
            "natsort==5.5.0\n",
            "nbconvert==5.5.0\n",
            "nbformat==4.4.0\n",
            "networkx==2.3\n",
            "nibabel==2.3.3\n",
            "nltk==3.2.5\n",
            "nose==1.3.7\n",
            "notebook==5.2.2\n",
            "np-utils==0.5.10.0\n",
            "numba==0.40.1\n",
            "numexpr==2.6.9\n",
            "numpy==1.16.3\n",
            "nvidia-ml-py3==7.352.0\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.0.1\n",
            "okgrade==0.4.3\n",
            "olefile==0.46\n",
            "opencv-contrib-python==3.4.3.18\n",
            "opencv-python==3.4.5.20\n",
            "openpyxl==2.5.9\n",
            "osqp==0.5.0\n",
            "packaging==19.0\n",
            "pandas==0.24.2\n",
            "pandas-datareader==0.7.0\n",
            "pandas-gbq==0.4.1\n",
            "pandas-profiling==1.4.1\n",
            "pandocfilters==1.4.2\n",
            "parso==0.4.0\n",
            "pathlib==1.0.1\n",
            "patsy==0.5.1\n",
            "pexpect==4.7.0\n",
            "pickleshare==0.7.5\n",
            "Pillow==4.3.0\n",
            "pip-tools==3.6.1\n",
            "plac==0.9.6\n",
            "plotly==3.6.1\n",
            "pluggy==0.7.1\n",
            "portpicker==1.2.0\n",
            "prefetch-generator==1.0.1\n",
            "preshed==2.0.1\n",
            "pretty-midi==0.2.8\n",
            "prettytable==0.7.2\n",
            "progressbar2==3.38.0\n",
            "prometheus-client==0.6.0\n",
            "promise==2.2.1\n",
            "prompt-toolkit==1.0.16\n",
            "protobuf==3.7.1\n",
            "psutil==5.4.8\n",
            "psycopg2==2.7.6.1\n",
            "ptyprocess==0.6.0\n",
            "py==1.8.0\n",
            "pyasn1==0.4.5\n",
            "pyasn1-modules==0.2.5\n",
            "pycocotools==2.0.0\n",
            "pycparser==2.19\n",
            "pydot==1.3.0\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "pyemd==0.5.1\n",
            "pyglet==1.3.2\n",
            "Pygments==2.1.3\n",
            "pygobject==3.26.1\n",
            "pymc3==3.6\n",
            "pymongo==3.8.0\n",
            "pymystem3==0.2.0\n",
            "PyOpenGL==3.1.0\n",
            "pyparsing==2.4.0\n",
            "pyrsistent==0.15.2\n",
            "pysndfile==1.3.2\n",
            "PySocks==1.7.0\n",
            "pystan==2.19.0.0\n",
            "pytest==3.6.4\n",
            "python-apt==1.6.3+ubuntu1\n",
            "python-chess==0.23.11\n",
            "python-dateutil==2.5.3\n",
            "python-louvain==0.13\n",
            "python-rtmidi==1.3.0\n",
            "python-slugify==3.0.2\n",
            "python-utils==2.3.0\n",
            "pytz==2018.9\n",
            "PyWavelets==1.0.3\n",
            "PyYAML==3.13\n",
            "pyzmq==17.0.0\n",
            "qtconsole==4.4.4\n",
            "regex==2018.1.10\n",
            "requests==2.21.0\n",
            "requests-oauthlib==1.2.0\n",
            "resampy==0.2.1\n",
            "retrying==1.3.3\n",
            "rpy2==2.9.5\n",
            "rsa==4.0\n",
            "s3fs==0.2.1\n",
            "s3transfer==0.2.0\n",
            "scikit-image==0.15.0\n",
            "scikit-learn==0.21.1\n",
            "scipy==1.3.0\n",
            "screen-resolution-extra==0.0.0\n",
            "scs==2.1.0\n",
            "seaborn==0.9.0\n",
            "semantic-version==2.6.0\n",
            "Send2Trash==1.5.0\n",
            "setuptools-git==1.2\n",
            "Shapely==1.6.4.post2\n",
            "simplegeneric==0.8.1\n",
            "six==1.12.0\n",
            "sklearn==0.0\n",
            "smart-open==1.8.3\n",
            "snowballstemmer==1.2.1\n",
            "sortedcontainers==2.1.0\n",
            "spacy==2.0.18\n",
            "Sphinx==1.8.5\n",
            "sphinxcontrib-websupport==1.1.0\n",
            "SQLAlchemy==1.3.3\n",
            "sqlparse==0.3.0\n",
            "stable-baselines==2.2.1\n",
            "statsmodels==0.9.0\n",
            "sympy==1.1.1\n",
            "tables==3.4.4\n",
            "tabulate==0.8.3\n",
            "tblib==1.4.0\n",
            "tensor2tensor==1.11.0\n",
            "tensorboard==1.13.1\n",
            "tensorboardcolab==0.0.22\n",
            "tensorflow==1.13.1\n",
            "tensorflow-estimator==1.13.0\n",
            "tensorflow-hub==0.4.0\n",
            "tensorflow-metadata==0.13.0\n",
            "tensorflow-probability==0.6.0\n",
            "termcolor==1.1.0\n",
            "terminado==0.8.2\n",
            "testpath==0.4.2\n",
            "text-unidecode==1.2\n",
            "textblob==0.15.3\n",
            "textgenrnn==1.4.1\n",
            "tfds-nightly==1.0.2.dev201905170105\n",
            "tflearn==0.3.2\n",
            "Theano==1.0.4\n",
            "thinc==6.12.1\n",
            "toolz==0.9.0\n",
            "torch==1.1.0\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.3.1\n",
            "torchvision==0.2.2.post3\n",
            "tornado==4.5.3\n",
            "tqdm==4.28.1\n",
            "traitlets==4.3.2\n",
            "tweepy==3.6.0\n",
            "typing==3.6.6\n",
            "tzlocal==1.5.1\n",
            "ujson==1.35\n",
            "umap-learn==0.3.8\n",
            "uritemplate==3.0.0\n",
            "urllib3==1.24.3\n",
            "vega-datasets==0.7.0\n",
            "wcwidth==0.1.7\n",
            "webencodings==0.5.1\n",
            "Werkzeug==0.15.4\n",
            "widgetsnbextension==3.4.2\n",
            "wordcloud==1.5.0\n",
            "wrapt==1.10.11\n",
            "xarray==0.11.3\n",
            "xgboost==0.82\n",
            "xkit==0.0.0\n",
            "xlrd==1.1.0\n",
            "xlwt==1.3.0\n",
            "yellowbrick==0.9.1\n",
            "zict==0.1.4\n",
            "zmq==0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcshbb8C7Ife",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "a3b24dd8-2a37-41e7-8516-dc78ab6df6d5"
      },
      "source": [
        "# フォルダ名が動的に生成されるので、都度変更が必要\n",
        "!saved_model_cli show --all --dir ./models/keras_export/1558534976"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['__saved_model_init_op']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['__saved_model_init_op'] tensor_info:\n",
            "        dtype: DT_INVALID\n",
            "        shape: unknown_rank\n",
            "        name: init_1\n",
            "  Method name is: \n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['dense_input'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 2)\n",
            "        name: dense_input:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['dense_1'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 1)\n",
            "        name: dense_1/Sigmoid:0\n",
            "  Method name is: tensorflow/serving/predict\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykO0_50XrcbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}